import os
import glob
import math
import numpy as np
import tensorflow as tf
import json
from kobert_tokenizer import KoBERTTokenizer

# ==============================================================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜ ë° GPU ì„¤ì •
# ==============================================================================

# 1. í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™” (í•„ìˆ˜: êµì°© ìƒíƒœ ë°©ì§€)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# 2. GPU ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹ (OOM ë°©ì§€)
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ… GPU ê°€ì† í™œì„±í™”ë¨: {len(gpus)}ê°œì˜ GPU ê°ì§€ë¨")
    except RuntimeError as e:
        print(f"GPU ì„¤ì • ì˜¤ë¥˜: {e}")
else:
    print("âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")

# ==============================================================================

# === [íŒŒë¼ë¯¸í„° ì„¤ì •] ===
BATCH_SIZE = 32  
EPOCHS = 100
LEARNING_RATE = 0.001
MAX_LEN = 64

# === [1. ë°ì´í„° ì œë„ˆë ˆì´í„°] ===
def data_generator(file_paths):
    for file_path in file_paths:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                info = data.get("sourceDataInfo", {})
                title = info.get("newsTitle", "")
                
                # ë³¸ë¬¸ ì• 5ë¬¸ì¥ ì¶”ì¶œ
                sentences = info.get("sentenceInfo", [])
                content_list = [item.get("sentenceContent", "") for item in sentences[:5]]
                body = " ".join(content_list)
                
                # ë¼ë²¨
                label_val = info.get("useType", 0)
                label = float(label_val)
                
                yield title, body, label

        except Exception as e:
            continue

# === [2. ë°ì´í„°ì…‹ íŒŒì´í”„ë¼ì¸ ìƒì„± í•¨ìˆ˜] ===
def create_dataset(data_path, tokenizer, max_len, batch_size):
    # 1. íŒŒì¼ íƒìƒ‰ (ì¬ê·€ì )
    print(f"ğŸ“‚ ê²½ë¡œ íƒìƒ‰ ì¤‘: {data_path}")
    search_pattern = os.path.join(data_path, "**", "*.json")
    all_files = glob.glob(search_pattern, recursive=True)
    
    file_count = len(all_files)
    print(f"   ã„´ ë°œê²¬ëœ íŒŒì¼ ìˆ˜: {file_count}ê°œ")
    
    if file_count == 0:
        return None, 0

    # 2. ì œë„ˆë ˆì´í„° ì—°ê²°
    def gen():
        yield from data_generator(all_files)

    dataset = tf.data.Dataset.from_generator(
        gen,
        output_signature=(
            tf.TensorSpec(shape=(), dtype=tf.string),
            tf.TensorSpec(shape=(), dtype=tf.string),
            tf.TensorSpec(shape=(), dtype=tf.float32)
        )
    )

    # 3. í† í¬ë‚˜ì´ì§• ë° ë§¤í•‘
    def tokenize_map(title, body, label):
        def py_tokenize(t, b):
            t_str = t.numpy().decode('utf-8')
            b_str = b.numpy().decode('utf-8')
            
            t_enc = tokenizer.encode_plus(t_str, max_length=max_len, padding='max_length', truncation=True)
            b_enc = tokenizer.encode_plus(b_str, max_length=max_len, padding='max_length', truncation=True)
            
            return np.array(t_enc['input_ids'], dtype=np.int32), np.array(b_enc['input_ids'], dtype=np.int32)

        title_ids, body_ids = tf.py_function(py_tokenize, [title, body], [tf.int32, tf.int32])
        
        title_ids.set_shape([max_len])
        body_ids.set_shape([max_len])
        
        return ({"title_input": title_ids, "body_input": body_ids}, label)

    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    dataset = dataset.map(tokenize_map, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
    return dataset, file_count

# === [3. ëª¨ë¸ êµ¬ì¡° ì •ì˜ (Siamese GRU)] ===
def build_siamese_gru_model(vocab_size, max_len, embed_dim=128, hidden_dim=64):
    input_title = tf.keras.Input(shape=(max_len,), name='title_input')
    input_body = tf.keras.Input(shape=(max_len,), name='body_input')

    embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_dim, mask_zero=True)
    gru_layer = tf.keras.layers.GRU(hidden_dim)

    vec_title = gru_layer(embedding_layer(input_title))
    vec_body = gru_layer(embedding_layer(input_body))

    # ì°¨ì´ ë²¡í„° ê³„ì‚° (L1 Distance)
    diff = tf.keras.layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([vec_title, vec_body])

    x = tf.keras.layers.Dense(32, activation='relu')(diff)
    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)

    return tf.keras.Model(inputs=[input_title, input_body], outputs=output)

# === [4. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡] ===
if __name__ == "__main__":
    
    print("\n=== í”„ë¡œê·¸ë¨ ì‹œì‘ ===")
    
    # 1. ê²½ë¡œ ì„¤ì •
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    TRAIN_PATH = os.path.join(BASE_DIR, "DataSet", "train")
    TEST_PATH = os.path.join(BASE_DIR, "DataSet", "test")

    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    try:
        tokenizer = KoBERTTokenizer.from_pretrained("skt/kobert-base-v1")
    except Exception as e:
        print(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        exit()

    # 3. [í•™ìŠµ] ë°ì´í„°ì…‹ ìƒì„±
    print("\n--- [Train] ë°ì´í„°ì…‹ ì¤€ë¹„ ---")
    if not os.path.exists(TRAIN_PATH):
        print(f"âŒ í•™ìŠµ ë°ì´í„° í´ë” ì—†ìŒ: {TRAIN_PATH}")
        exit()
        
    train_ds, train_files = create_dataset(TRAIN_PATH, tokenizer, MAX_LEN, BATCH_SIZE)
    
    if train_ds is None:
        print("âŒ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    train_steps = math.ceil(train_files / BATCH_SIZE)
    print(f"   ã„´ í•™ìŠµ ìŠ¤í… ìˆ˜: {train_steps} (ì´ {train_files}ê°œ)")

    # 4. ëª¨ë¸ ìƒì„± ë° ì»´íŒŒì¼
    print("\n--- ëª¨ë¸ ë¹Œë“œ ---")
    model = build_siamese_gru_model(vocab_size=tokenizer.vocab_size, max_len=MAX_LEN)
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    # 5. ëª¨ë¸ í•™ìŠµ
    print("\n=== í•™ìŠµ ì‹œì‘ ===")
    try:
        history = model.fit(
            train_ds,
            epochs=EPOCHS,
            steps_per_epoch=train_steps,
            verbose=1
        )
        print("=== í•™ìŠµ ì™„ë£Œ ===")
        
        # [ìˆ˜ì •ë¨] .keras í¬ë§·ìœ¼ë¡œ ì €ì¥ (Warning í•´ê²°)
        save_name = "siamese_gru_model.keras"
        model.save(save_name)
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_name}")
        
    except KeyboardInterrupt:
        print("\nâ›” ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        exit()
    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        exit()

    # 6. [í‰ê°€] í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
    print("\n=== ëª¨ë¸ í‰ê°€ (Test) ===")
    
    if not os.path.exists(TEST_PATH):
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ í´ë”ê°€ ì—†ì–´ í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤: {TEST_PATH}")
    else:
        print(f"--- [Test] ë°ì´í„°ì…‹ ì¤€ë¹„ ---")
        test_ds, test_files = create_dataset(TEST_PATH, tokenizer, MAX_LEN, BATCH_SIZE)
        
        if test_ds is not None:
            test_steps = math.ceil(test_files / BATCH_SIZE)
            print(f"   ã„´ í…ŒìŠ¤íŠ¸ ìŠ¤í… ìˆ˜: {test_steps} (ì´ {test_files}ê°œ)")
            
            print("\n--- í‰ê°€ ì§„í–‰ ì¤‘... ---")
            # evaluate í•¨ìˆ˜ë¡œ ì†ì‹¤ê³¼ ì •í™•ë„ ê³„ì‚°
            test_loss, test_acc = model.evaluate(test_ds, steps=test_steps, verbose=1)
            
            print("\nğŸ“Š [ìµœì¢… í‰ê°€ ê²°ê³¼]")
            print(f"   - Test Loss    : {test_loss:.4f}")
            print(f"   - Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")
        else:
            print("âš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼(.json)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")