{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1001c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# 기존 파일에서 데이터 로더 함수 가져오기 (가정)\n",
    "# from json2array import extract_value \n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# === [설정] ===\n",
    "DATA_PATH = \"SampleData\" # 데이터 경로\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_LEN = 64\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "def extract_value(path):\n",
    "\n",
    "    newsTitles, newsContents, useTypes = [], [], []\n",
    "    \n",
    "    # 디렉토리가 실제로 존재하는지 확인\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"오류: '{path}' 경로를 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # 디렉토리 내의 파일 목록을 순회\n",
    "    for filename in os.listdir(path):\n",
    "\n",
    "        # .json 확인\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(path, filename)\n",
    "            \n",
    "            try:\n",
    "                # 파일 열기\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    _data = json.load(f)\n",
    "\n",
    "                    newsTitles.append(_data[\"sourceDataInfo\"][\"newsTitle\"])\n",
    "\n",
    "                    _content_list = [item.get(\"sentenceContent\", \"\") for item in _data[\"sourceDataInfo\"][\"sentenceInfo\"][:5]]\n",
    "                    newsContents.append(\" \".join(_content_list))\n",
    "                    \n",
    "                    useTypes.append(_data[\"sourceDataInfo\"][\"useType\"])\n",
    "\n",
    "                    \n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"오류: {filename} 파일은 올바른 JSON 형식이 아닙니다.\")\n",
    "            except Exception as e:\n",
    "                print(f\"오류: {filename} 처리 중 문제 발생 - {e}\")\n",
    "\n",
    "\n",
    "    return newsTitles, newsContents, useTypes\n",
    "\n",
    "def preprocess_data(titles, bodies, labels, tokenizer, max_len):\n",
    "    title_ids = []\n",
    "    body_ids = []\n",
    "    \n",
    "    for t, b in zip(titles, bodies):\n",
    "        t_enc = tokenizer.encode_plus(t, add_special_tokens=True, max_length=max_len, \n",
    "                                      padding='max_length', truncation=True)\n",
    "        b_enc = tokenizer.encode_plus(b, add_special_tokens=True, max_length=max_len, \n",
    "                                      padding='max_length', truncation=True)\n",
    "        title_ids.append(t_enc['input_ids'])\n",
    "        body_ids.append(b_enc['input_ids'])\n",
    "\n",
    "\n",
    "\n",
    "    return (np.array(title_ids, dtype=np.int32), \n",
    "            np.array(body_ids, dtype=np.int32), \n",
    "            np.array(labels, dtype=np.float32))\n",
    "\n",
    "def build_siamese_gru_model(vocab_size, max_len, embed_dim=128, hidden_dim=64):\n",
    "    input_title = tf.keras.Input(shape=(max_len,), name='title_input')\n",
    "    input_body = tf.keras.Input(shape=(max_len,), name='body_input')\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
    "    gru_layer = tf.keras.layers.GRU(hidden_dim)\n",
    "\n",
    "    vec_title = gru_layer(embedding_layer(input_title))\n",
    "    vec_body = gru_layer(embedding_layer(input_body))\n",
    "\n",
    "    print(vec_body)\n",
    "    print(vec_title)\n",
    "\n",
    "    # 차분 층\n",
    "    diff = tf.keras.layers.Lambda(lambda x: tf.abs(x[0] - x[1]))([vec_title, vec_body])\n",
    "\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(diff)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=[input_title, input_body], outputs=output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    target_path = Path(\"SampleData\").resolve()\n",
    "    raw_titles, raw_contents, raw_labels = extract_value(target_path)\n",
    "    \n",
    "    # 데이터 전처리 (Numpy 변환)\n",
    "    tokenizer = KoBERTTokenizer.from_pretrained(\"skt/kobert-base-v1\")\n",
    "    X_title, X_body, y = preprocess_data(raw_titles, raw_contents, raw_labels, tokenizer, MAX_LEN)\n",
    "    train_X = [X_title, X_body]\n",
    "\n",
    "    # 모델 로드\n",
    "    model = build_siamese_gru_model(vocab_size=tokenizer.vocab_size, max_len=MAX_LEN)\n",
    "    \n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    # 5. 학습 진행 (History 객체에 기록 저장)\n",
    "    print(\"\\n=== 학습 시작 ===\")\n",
    "    history = model.fit(\n",
    "        x=train_X,  # 입력이 2개이므로 리스트로 전달\n",
    "        y=y,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=0.2, # 검증 데이터 활용\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"=== 학습 완료 ===\")\n",
    "\n",
    "    # 테스트 (예측)\n",
    "    print(\"\\n=== 테스트 수행 ===\")\n",
    "    test_title_raw = [\"충격! 로또 1등 당첨 비결 공개\"]\n",
    "    test_body_raw = [\"로또는 독립시행이므로 비결은 없습니다. 운에 맡기세요.\"]\n",
    "    \n",
    "    # 테스트 데이터 전처리\n",
    "    X_test_t, X_test_b, _ = preprocess_data(test_title_raw, test_body_raw, [0], tokenizer, MAX_LEN)\n",
    "    \n",
    "    # 예측 수행\n",
    "    prediction = model.predict([X_test_t, X_test_b])\n",
    "    score = prediction[0][0]\n",
    "    result = \"낚시성 기사(불일치)\" if score > 0.5 else \"정상 기사(일치)\"\n",
    "    \n",
    "    print(f\"\\n[테스트 결과]\\n제목: {test_title_raw[0]}\\n본문: {test_body_raw[0]}\\n\\n판정: {result} (점수: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f9d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# 기존 파일에서 데이터 로더 함수 가져오기 (가정)\n",
    "# from json2array import extract_value \n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "# === [설정] ===\n",
    "DATA_PATH = \"SampleData\" # 데이터 경로\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_LEN = 64\n",
    "\n",
    "import os\n",
    "import json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
